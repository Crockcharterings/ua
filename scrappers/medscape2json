#!/usr/bin/python3

import sys
import json
import scraplib
import time

scraplib.open_cookies()

def medscape_article(doc, url):
	doc = yield from doc.fetch(url)
	print(json.dumps({
		"title": doc("#articleHead h1").text(),
		"body": doc("#articleContent").html(),
		"author": doc("#authors").text(),
		"id": doc.base_url,
		"url": doc.base_url,
		"host": "medscape.com"
	}))

def medscape_articles(url, type):
	doc = yield from scraplib.fetch(url + "/" + type)
	articles = set(_.url() for _ in doc('a[href*="/viewarticle"]').pq())
	yield from scraplib.wait(medscape_article(doc, _) for _ in articles)

def do_login(url, login, password):
	data = {"userId": login, "password": password}
	resp = yield from scraplib.fetch("https://login.medscape.com/login/sso/login", "POST", data = data, allow_redirects = False, raw_response = True, cookies = {"s_sq": "[[B]]"})
	if resp.status == 200:
		print("Bad credentials", file = sys.stderr)
		sys.exit(1)

def medscape_index(url, login, password):
	yield from do_login(url, login, password)
	url = url.rstrip("/")
	yield from scraplib.wait([medscape_articles(url, "news"), medscape_articles(url, "journals")])

if len(sys.argv) < 4 or "-h" in sys.argv or "--help" in sys.argv:
	print("Usage: %s speciality-index-url login password" % sys.argv[0])
	sys.exit(1)

scraplib.main(medscape_index(sys.argv[1], sys.argv[2], sys.argv[3]))
