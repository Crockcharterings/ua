#!/usr/bin/python3

import scraplib
import sys
import json
import time
import re
import urllib.parse

scraplib.open_cookies("~/.cache/ipboard2json/cookies")

def is_logged_in(doc):
	return doc("#user_navigation").hasClass("logged_in")

def fetch(doc, url, *args, **kwargs):
	doc = yield from doc.fetch(url, *args, **kwargs)
	assert is_logged_in(doc)
	return doc

def login(doc, user, password):
	doc = yield from doc.fetch(doc("a#sign_in").url())
	data = {"auth_key": doc('input[name="auth_key"]').val(), "rememberMe": "1", "ips_username": user, "ips_password": password}
	resp = yield from doc.fetch(doc("form#login").url(), method = "POST", data = data, allow_redirects = False, raw_response = True)
	if resp.status == 200:
		print("Bad credentials\n", file = sys.stderr)
		sys.exit(1)
	url = scraplib.urljoin(doc.base_url, dict(resp.message.headers)["LOCATION"])
	doc = yield from doc.fetch(url)
	assert is_logged_in(doc)
	return doc

def scrap_page(doc):
	topic_id = re.search(r"topic/(\d+)", doc.base_url).group(1)
	first_message = re.search(r"page-(\d+)", doc.base_url)
	first_message = (not first_message) or (int(first_message.group(1)) == 1)
	for entry in doc(".post_block").pq():
		prefix = (not first_message) and "Re: " or ""
		post_id = entry.attr('id')
		date = entry("abbr.published").attr('title')
		date = time.strptime(re.sub(r"(?<=[+-]\d\d):(?=\d\d$)", "", date), "%Y-%m-%dT%H:%M:%S%z")
		entry(".post_body .post_controls").remove()
		entry(".post_body .signature").remove()
		entry(".post_body .posted_info").remove()

		post_body = entry(".post_body").html().replace("<blockquote", '<blockquote type="cite"')

		body = '''
			<div>
			<div style="text-align:center;float:right;margin:10px;z-index:100;position:relative">
			  %s<br><img src="%s"><br><a href="%s"">View post</a>
			</div>
			%s
			<div style="clear:both"></div>
			</div>
		''' % (entry(".vcard").text(), entry(".ipsUserPhoto").url(), entry(".post_id a").url(), post_body)

		print(json.dumps({
			"title": prefix + doc(".ipsType_pagetitle").text(),
			"date": time.strftime("%d %b %Y %H:%M:%S %z", date),
			"body": body,
			"author": entry(".vcard").text(),
			"id": "ua.ipboard."+(first_message and topic_id or post_id),
			"references": ["ua.ipboard."+topic_id],
			"url": doc.base_url,
			"host": urllib.parse.urlsplit(doc.base_url).netloc
		}))
		first_message = False

def scrap_topic(doc, url):
	def pagenum(page):
		match = re.search(r"page-(\d+)", page)
		return match and int(match.group(1)) or 1
	doc = yield from doc.fetch(url)
	scrap_page(doc)

	nextpages = []
	current_page = pagenum(doc("li.page.active a").url())
	last_page = doc('link[rel="last"]')
	if last_page:
		last_page = pagenum(last_page.url())
		for i in range(current_page, last_page+1):
			nextpages.append(re.sub(r"page-\d+", "page-%d" % i, doc('link[rel="last"]').url()))
	
	for page in (yield from scraplib.wait(doc.fetch(p) for p in nextpages)):
		scrap_page(page.result())

def scrap_forum(doc, url):
	# TODO: handle "unread topic on next page" case
	doc = yield from fetch(doc, url)
	yield from scraplib.wait(scrap_topic(doc, t.url()) for t in doc("#forum_table .unread .col_f_icon a").pq())

def scrap_ipb(url, user, password):
	doc = yield from scraplib.fetch(url)
	if not is_logged_in(doc):
		print("Logging in...", file = sys.stderr)
		doc = yield from login(doc, user, password)

	yield from scraplib.wait(scrap_forum(doc, f.url()) for f in doc("#board_index .unread .col_c_forum a").pq())

import locale
locale.setlocale(locale.LC_ALL, "C")

if len(sys.argv) < 4 or "-h" in sys.argv or "--help" in sys.argv:
	print("Usage: %s board-index-url username password" % sys.argv[0])
	sys.exit(1)

scraplib.main(scrap_ipb(sys.argv[1], sys.argv[2], sys.argv[3]))
